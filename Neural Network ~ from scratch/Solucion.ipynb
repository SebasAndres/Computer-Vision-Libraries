{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programando una red neuronal desde 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase RedNeuronal\n",
    "\n",
    "Sea $C(a)$ la funcion del coste definida como...\n",
    "$$C=\\frac{1}{2}\\sum_i (y_i - a_i)^2$$\n",
    "\n",
    "Con la funcion de activacion sigmoidea $a(Z^L)$...\n",
    "$$a(z) = \\frac{1}{a+e^{-z}}$$\n",
    "\n",
    "Para la sumatoria $Z$ de la capa $L$...\n",
    "$$Z^L = \\sum w_i^L x_i^{L-1} + B^L$$\n",
    "\n",
    "### Forward Propagation:\n",
    "\n",
    "El output de la L-esima capa es:\n",
    "\n",
    "$$ out^{L} = a(Z^{L}) = a(W^{L}a^{L-1} + b^{L}) = a(\\sum w_i^{L} x_i + b^{L}) $$\n",
    "\n",
    "Donde $a^{L-1}$ es el output de la capa anterior o, en su defecto, el input.\n",
    "\n",
    "### Backward Propagation:\n",
    "\n",
    "Sea $\\delta^L$ el error imputado de la neurona:\n",
    "\n",
    "$$\\delta^L = \\frac{\\partial C}{\\partial a} * \\frac{\\partial a}{\\partial Z^L}$$\n",
    "\n",
    "Vemos el caso de la ultima capa, por regla de la cadena:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^L} = \\delta^L * \\frac{\\partial Z^L}{\\partial W^L}$$\n",
    "$$\\frac{\\partial C}{\\partial B^L} = \\delta^L * \\frac{\\partial Z^L}{\\partial B}$$\n",
    "\n",
    "En particular, valen las derivadas:\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial B} = 1$$\n",
    "$$\\frac{\\partial Z}{\\partial W^L} = a^{L-1}$$\n",
    "\n",
    "Entonces nos queda:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^L} = \\delta^L * a^{L-1}$$\n",
    "$$\\frac{\\partial C}{\\partial B^L} = \\delta^L $$\n",
    "\n",
    "Y si L es la primer capa $\\rightarrow a^{L-1}$ es el input.\n",
    "\n",
    "Luego, para las capas anteriores:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{L-1}} = \\delta^L * \\frac{\\partial Z^L}{\\partial a^{L-1}}\n",
    " * \\frac{\\partial a^{L-1}}{\\partial Z^{L-1}} * \\frac{\\partial Z^{L-1}}{\\partial W^{L-1}} $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial B^{L-1}} = \\frac{\\partial Z^L}{\\partial a^{L-1}}\n",
    " * \\frac{\\partial a^{L-1}}{\\partial Z^{L-1}} * \\frac{\\partial Z^{L-1}}{\\partial B^{L-1}}$$\n",
    "\n",
    "Y por regla de la cadena queda:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{L-1}} = \\delta^L * W^L * \\frac{\\partial a^{L-1}}{\\partial z^{L-1}} * a^{L-2} $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^{L-1}} = \\delta^L * W^L * \\frac{\\partial a^{L-1}}{\\partial z^{L-1}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, neurons, input_shape, activation):\n",
    "        self.neurons = neurons\n",
    "        self.input_shape = input_shape\n",
    "        self.weights = np.random.rand(self.input_shape, self.neurons)\n",
    "        self.bias = np.random.rand(self.neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.activation(np.dot(inputs, self.weights) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, input_size, layers, l_rate, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.layers = layers\n",
    "        self.output_size = output_size  \n",
    "        self.l_rate = l_rate\n",
    "\n",
    "    def forward(self, x, retLayers=False):\n",
    "        # Forward pass del modelo\n",
    "        t = x\n",
    "        layer_outputs = []\n",
    "        for layer in self.layers:\n",
    "            t = layer.forward(t)\n",
    "            layer_outputs.append(t)\n",
    "\n",
    "        if retLayers:\n",
    "            return t, layer_outputs\n",
    "\n",
    "        return t          \n",
    "\n",
    "    def train(self, X, Y):        \n",
    "\n",
    "        # Entreno el modelo\n",
    "        \n",
    "        for i, x_ in enumerate(X):\n",
    "\n",
    "            x = x.reshape(784,1)\n",
    "\n",
    "            # Forwardpropagation:\n",
    "            out, a = self.forward(x, retLayers=True) # output_layers\n",
    "\n",
    "            # Backpropagation:\n",
    "            \n",
    "            # Calculo el gradiente de la ultima capa\n",
    "            L = len(self.layers)\n",
    "            error_L = -(Y[i] - a[L]) * e**(-a[L])/(1+e**(-a[L]))**2\n",
    "            W_grad_L = error_L * a[L-1]\n",
    "            B_grad_L = error_L\n",
    "\n",
    "            # Actualizo los pesos de la ultima capa\n",
    "            self.layers[-1].weights -= self.l_rate * W_grad_L\n",
    "            self.layers[-1].bias -= self.l_rate * B_grad_L\n",
    "\n",
    "            # Repito el procedimiento en las capas anteriores \n",
    "            for j in range(L-1, 0, -1):\n",
    "                l = L - j # capa actual \n",
    "                da = e**(-a[l])/(1+e**(-a[l]))**2 # derivada de la activacion\n",
    "                # Calculo el gradiente de la capa actual\n",
    "                W_grad_j = error_L * self.layers[l+1].weights * da * a[l-1]\n",
    "                B_grad_j = error_L * self.layers[l+1].weights * da\n",
    "                # Actualizo los pesos de la capa actual\n",
    "                self.layers[j].weights -= self.l_rate * W_grad_j\n",
    "                self.layers[j].bias -= self.l_rate * B_grad_j\n",
    "            \n",
    "            pass\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99994666, 0.99989188, 0.99942884, 0.99998529, 0.99986706,\n",
       "       0.99998856, 0.99991913, 0.9999316 , 0.99945283, 0.999824  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "                784,\n",
    "                [Layer(16, 784, lambda x: 1/(1+e**(-x))),\n",
    "                 Layer(16, 16, lambda x: 1/(1+e**(-x))),\n",
    "                 Layer(10, 16, lambda x: 1/(1+e**(-x)))],\n",
    "                0.1,\n",
    "                10)\n",
    "\n",
    "model.forward(np.random.rand(784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = X_train[0].reshape(784,)\n",
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (784,16) and (784,) not aligned: 16 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49mdot(model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mweights, sample_x) \u001b[39m+\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbias \n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (784,16) and (784,) not aligned: 16 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(model.layers[0].weights, sample_x) + model.layers[0].bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
